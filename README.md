# Backend ‚Äì Node.js + Ollama

Este reposit√≥rio cont√©m a API de backend, que utiliza o modelo `llama3` da Ollama para responder perguntas relacionadas ao time FURIA Esports no cen√°rio de Counter-Strike.

## üõ†Ô∏è Instala√ß√£o e Execu√ß√£o Local

### 1. **Pr√©-requisitos**
- [Node.js](https://nodejs.org/) instalado.
- [Ollama](https://ollama.com/download) instalado e funcionando localmente.
- Modelo `llama3` carregado no Ollama:

```bash
ollama run llama3
```

### 2. **Clone o reposit√≥rio**

```bash
git clone https://github.com/furlanettoeduardo/chatbot-furia-backend/
cd chatbot-furia-backend
```

### 2. **Instale as depend√™ncias**

```bash
npm install
```

### 2. **Inicie o servidor**

```bash
node server.js
```

O backend ser√° iniciado na porta 3000. A API estar√° acess√≠vel em:

http://localhost:3000/chat

O formato da requisi√ß√£o √© de envio:

```bash
{
  "prompt": "Qual √© o elenco atual da FURIA?"
}
```

E de devolu√ß√£o:

```bash
{
  "response": "Resposta gerada pelo modelo"
}
```
üö® Aviso
Certifique-se de que o Ollama esteja rodando e o modelo llama3 esteja carregado corretamente para que o backend funcione conforme esperado.
